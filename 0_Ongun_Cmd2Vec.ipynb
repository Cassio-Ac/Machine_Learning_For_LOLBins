{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Living-Off-The-Land Command Detection Using Active Learning\n",
    "\n",
    "```\n",
    "@Conference{Ongun2021,\n",
    "  author     = {Ongun, Talha and Stokes, Jack W. and Or, Jonathan Bar and Tian, Ke and Tajaddodianfar, Farid and Neil, Joshua and Seifert, Christian and Oprea, Alina and Platt, John C.},\n",
    "  booktitle  = {Proceedings of the 24th International Symposium on Research in Attacks, Intrusions and Defenses},\n",
    "  title      = {Living-Off-The-Land Command Detection Using Active Learning},\n",
    "  year       = {2021},\n",
    "  address    = {New York, NY, USA},\n",
    "  month      = {10},\n",
    "  pages      = {442–455},\n",
    "  publisher  = {Association for Computing Machinery},\n",
    "  series     = {RAID '21},\n",
    "  abstract   = {In recent years, enterprises have been targeted by advanced adversaries who leverage creative ways to infiltrate their systems and move laterally to gain access to critical data. One increasingly common evasive method is to hide the malicious activity behind a benign program by using tools that are already installed on user computers. These programs are usually part of the operating system distribution or another user-installed binary, therefore this type of attack is called “Living-Off-The-Land”. Detecting these attacks is challenging, as adversaries may not create malicious files on the victim computers and anti-virus scans fail to detect them. We propose the design of an Active Learning framework called LOLAL for detecting Living-Off-the-Land attacks that iteratively selects a set of uncertain and anomalous samples for labeling by a human analyst. LOLAL is specifically designed to work well when a limited number of labeled samples are available for training machine learning models to detect attacks. We investigate methods to represent command-line text using word-embedding techniques, and design ensemble boosting classifiers to distinguish malicious and benign samples based on the embedding representation. We leverage a large, anonymized dataset collected by an endpoint security product and demonstrate that our ensemble classifiers achieve an average F1 score of 96% at classifying different attack classes. We show that our active learning method consistently improves the classifier performance, as more training data is labeled, and converges in less than 30 iterations when starting with a small number of labeled instances.},\n",
    "  day        = {7},\n",
    "  doi        = {10.1145/3471621.3471858},\n",
    "  isbn       = {9781450390583},\n",
    "  keywords   = {Active learning for security, Advanced Persistent Threats, Contextual text embeddings, Threat detection},\n",
    "  location   = {San Sebastian, Spain},\n",
    "  pagetotal  = {14},\n",
    "  priority   = {prio1},\n",
    "  ranking    = {rank5},\n",
    "  readstatus = {read},\n",
    "  relevance  = {relevant},\n",
    "  url        = {https://doi.org/10.1145/3471621.3471858},\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Token creation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dados = [\"git commit --am _STRING_\",\"echo at http:_PATH_ my email is _EMAIL_ and my website http:_PATH_ go to ftp:_PATH_\", \n",
    "\"ping _IP_ && ping google.com\",\n",
    "\"rm _PATH_ | rm _PATH_  |  rm _PATH_ & cp _PATH_ _PATH_\",\n",
    "\"chmod +x xampp-linux-x64-5.6.33-0-installer.run | bash x.sh\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['git', 'commit', '--am', '_STRING_']\n",
      "['echo', 'at', 'http:_PATH_', 'my', 'email', 'is', '_EMAIL_', 'and', 'my', 'website', 'http:_PATH_', 'go', 'to', 'ftp:_PATH_']\n",
      "['ping', '_IP_', '&&', 'ping', 'google.com']\n",
      "['rm', '_PATH_', '|', 'rm', '_PATH_', '|', 'rm', '_PATH_', '&', 'cp', '_PATH_', '_PATH_']\n",
      "['chmod', '+x', 'xampp-linux-x64-5.6.33-0-installer.run', '|', 'bash', 'x.sh']\n"
     ]
    }
   ],
   "source": [
    "lista = []\n",
    "for i in Dados:\n",
    "    print(i.split())\n",
    "    lista.append(i.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['git', 'commit', '--am', '_STRING_'], ['echo', 'at', 'http:_PATH_', 'my', 'email', 'is', '_EMAIL_', 'and', 'my', 'website', 'http:_PATH_', 'go', 'to', 'ftp:_PATH_'], ['ping', '_IP_', '&&', 'ping', 'google.com'], ['rm', '_PATH_', '|', 'rm', '_PATH_', '|', 'rm', '_PATH_', '&', 'cp', '_PATH_', '_PATH_'], ['chmod', '+x', 'xampp-linux-x64-5.6.33-0-installer.run', '|', 'bash', 'x.sh']]\n"
     ]
    }
   ],
   "source": [
    "print(lista)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submission of Tokens to vectorization models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, FastText\n",
    "\n",
    "# Sua lista\n",
    "lista = [['git', 'commit', '--am', '_STRING_'], ['echo', 'at', 'http:_PATH_', 'my', 'email', 'is', '_EMAIL_', 'and', 'my', 'website', 'http:_PATH_', 'go', 'to', 'ftp:_PATH_'], ['ping', '_IP_', '&&', 'ping', 'google.com'], ['rm', '_PATH_', '|', 'rm', '_PATH_', '|', 'rm', '_PATH_', '&', 'cp', '_PATH_', '_PATH_'], ['chmod', '+x', 'xampp-linux-x64-5.6.33-0-installer.run', '|', 'bash', 'x.sh']]\n",
    "\n",
    "# Treina o modelo Word2Vec\n",
    "word2vec_model = Word2Vec(sentences=lista, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Treina o modelo FastText\n",
    "fasttext_model = FastText(sentences=lista, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Exemplo de uso dos modelos treinados\n",
    "vector_w2v = word2vec_model.wv['git']\n",
    "vector_ft = fasttext_model.wv['git']\n",
    "\n",
    "#print(vector_w2v)\n",
    "#print(vector_ft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is created ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token: git, Mean Leaf Probability for Label 1 (Normalized): 0.1401111111111111\n",
      "Token: commit, Mean Leaf Probability for Label 1 (Normalized): 0.8406666666666667\n",
      "Token: --am, Mean Leaf Probability for Label 1 (Normalized): 0.09340740740740741\n",
      "Token: _STRING_, Mean Leaf Probability for Label 1 (Normalized): 0.10508333333333333\n",
      "Token: ping, Mean Leaf Probability for Label 1 (Normalized): 0.12881944444444446\n",
      "Token: _IP_, Mean Leaf Probability for Label 1 (Normalized): 0.1766666666666667\n",
      "Token: &&, Mean Leaf Probability for Label 1 (Normalized): 0.09814814814814815\n",
      "Token: google.com, Mean Leaf Probability for Label 1 (Normalized): 0.0803030303030303\n",
      "Token: chmod, Mean Leaf Probability for Label 1 (Normalized): 0.06\n",
      "Token: +x, Mean Leaf Probability for Label 1 (Normalized): 0.012\n",
      "Token: xampp-linux-x64-5.6.33-0-installer.run, Mean Leaf Probability for Label 1 (Normalized): 0.017142857142857144\n",
      "Token: |, Mean Leaf Probability for Label 1 (Normalized): 0.02\n",
      "Token: bash, Mean Leaf Probability for Label 1 (Normalized): 0.010909090909090908\n",
      "Token: x.sh, Mean Leaf Probability for Label 1 (Normalized): 0.015\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "# Labels correspondentes a cada entrada\n",
    "labels = [1, 0, 1, 0, 1]  # Supondo que você tem os rótulos para cada entrada\n",
    "\n",
    "# Obtém os vetores de palavras do Word2Vec\n",
    "word_vectors = word2vec_model.wv\n",
    "\n",
    "# Obtém vetores de palavras para cada token na lista de tokens\n",
    "entry_vectors = []\n",
    "entry_labels = []\n",
    "for i, tokens in enumerate(lista_de_tokens):\n",
    "    for token in tokens:\n",
    "        if token in word_vectors:\n",
    "            entry_vectors.append(word_vectors[token])\n",
    "            entry_labels.append(labels[i])\n",
    "\n",
    "# Treina o modelo RandomForest com os vetores das palavras\n",
    "random_forest_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "random_forest_model.fit(entry_vectors, entry_labels)\n",
    "\n",
    "# Obtém o número de entradas na árvore para cada token com label 1\n",
    "num_samples_per_leaf = random_forest_model.apply(entry_vectors)\n",
    "\n",
    "# Cria um dicionário para armazenar a soma das probabilidades das folhas para cada token com label 1\n",
    "token_leaf_probabilities_sum = {}\n",
    "\n",
    "# Associa cada token com a soma das probabilidades das folhas correspondentes (apenas para tokens com label 1)\n",
    "for i, tokens in enumerate(lista_de_tokens):\n",
    "    if labels[i] == 1:  # Considera apenas tokens com label 1\n",
    "        for j, token in enumerate(tokens):\n",
    "            if token not in token_leaf_probabilities_sum:\n",
    "                token_leaf_probabilities_sum[token] = []\n",
    "            token_leaf_probabilities_sum[token].append(random_forest_model.predict_proba([entry_vectors[i]])[0][1] / num_samples_per_leaf[i, j])\n",
    "\n",
    "# Calcula a média das probabilidades das folhas para cada token com label 1\n",
    "mean_leaf_probabilities_label_1_normalized = {token: np.mean(probs) for token, probs in token_leaf_probabilities_sum.items()}\n",
    "\n",
    "# Exibe a média das probabilidades das folhas para cada token com label 1, normalizada pelo número de entradas na árvore\n",
    "for token, mean_prob in mean_leaf_probabilities_label_1_normalized.items():\n",
    "    print(f\"Token: {token}, Probabilidade Média da Leaf para Label 1 (Normalized): {mean_prob}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "\n",
    "for i, tokens in enumerate(lista_de_tokens):\n",
    "    word_vecs = [word_vectors[token] for token in tokens if token in word_vectors.key_to_index]\n",
    "    \n",
    "    if len(word_vecs) > 0:\n",
    "        word_vecs = np.array(word_vecs)\n",
    "        \n",
    "        if len(word_vecs) >= 3:\n",
    "            min_values = np.min(word_vecs, axis=0)[:3]\n",
    "            max_values = np.max(word_vecs, axis=0)[:3]\n",
    "            avg_values = np.mean(word_vecs, axis=0)[:3]\n",
    "        else:\n",
    "            min_values = np.zeros(3)\n",
    "            max_values = np.zeros(3)\n",
    "            avg_values = np.zeros(3)\n",
    "        \n",
    "        token_scores = [mean_leaf_probabilities_label_1_normalized[token] for token in tokens if token in mean_leaf_probabilities_label_1_normalized]\n",
    "        token_scores.sort(reverse=True)\n",
    "        max_scores = token_scores[:3]\n",
    "        \n",
    "        num_tokens = len(tokens)\n",
    "        \n",
    "        rare_count = sum(1 for token in tokens if token not in word_vectors.key_to_index or word_vectors.get_vecattr(token, 'count') <= 1)\n",
    "        \n",
    "        label = labels[i]\n",
    "        \n",
    "        entry_data = np.array((min_values[0] if len(min_values) > 0 else 0, min_values[1] if len(min_values) > 1 else 0, min_values[2] if len(min_values) > 2 else 0,\n",
    "                                max_values[0] if len(max_values) > 0 else 0, max_values[1] if len(max_values) > 1 else 0, max_values[2] if len(max_values) > 2 else 0,\n",
    "                                avg_values[0] if len(avg_values) > 0 else 0, avg_values[1] if len(avg_values) > 1 else 0, avg_values[2] if len(avg_values) > 2 else 0,\n",
    "                                max_scores[0] if len(max_scores) > 0 else 0, max_scores[1] if len(max_scores) > 1 else 0, max_scores[2] if len(max_scores) > 2 else 0,\n",
    "                                num_tokens, rare_count, label),\n",
    "                              dtype=[('min_val_0', np.float64),\n",
    "                                     ('min_val_1', np.float64),\n",
    "                                     ('min_val_2', np.float64),\n",
    "                                     ('max_val_0', np.float64),\n",
    "                                     ('max_val_1', np.float64),\n",
    "                                     ('max_val_2', np.float64),\n",
    "                                     ('avg_val_0', np.float64),\n",
    "                                     ('avg_val_1', np.float64),\n",
    "                                     ('avg_val_2', np.float64),\n",
    "                                     ('max_score_0', np.float64),\n",
    "                                     ('max_score_1', np.float64),\n",
    "                                     ('max_score_2', np.float64),\n",
    "                                     ('num_tokens', np.int64),\n",
    "                                     ('rare_count', np.int64),\n",
    "                                     ('label', np.int64)])\n",
    "        \n",
    "        new_dataset.append(entry_data)\n",
    "\n",
    "new_dataset = np.array(new_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([(-0.00950012, -0.00932806, -0.00777076, 0.00769665, 0.00956222, 0.00216339, -0.00331968,  0.00339692, -0.00158609, 0.84066667, 0.14011111, 0.10508333,  4,  4, 1),\n",
       "       (-0.00957855, -0.00980292, -0.00717673, 0.00977506, 0.00894312, 0.00944711,  0.00038211, -0.00025323,  0.00093848, 0.        , 0.        , 0.        , 14, 10, 0),\n",
       "       (-0.00823815, -0.00590276, -0.00019391, 0.00133212, 0.00930552, 0.00998768, -0.00357826,  0.00469329,  0.0038395 , 0.17666667, 0.12881944, 0.12881944,  5,  3, 1),\n",
       "       (-0.00861682, -0.00128281, -0.00680692, 0.00964721, 0.00732885, 0.00519266, -0.00196912,  0.00203336,  0.0026725 , 0.02      , 0.02      , 0.        , 12,  2, 0),\n",
       "       (-0.00696548, -0.00666834, -0.00943299, 0.00835129, 0.00573516, 0.00183118,  0.0006781 ,  0.00076893, -0.00538804, 0.06      , 0.02      , 0.01714286,  6,  5, 1)],\n",
       "      dtype=[('min_val_0', '<f8'), ('min_val_1', '<f8'), ('min_val_2', '<f8'), ('max_val_0', '<f8'), ('max_val_1', '<f8'), ('max_val_2', '<f8'), ('avg_val_0', '<f8'), ('avg_val_1', '<f8'), ('avg_val_2', '<f8'), ('max_score_0', '<f8'), ('max_score_1', '<f8'), ('max_score_2', '<f8'), ('num_tokens', '<i8'), ('rare_count', '<i8'), ('label', '<i8')])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_dataset"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
